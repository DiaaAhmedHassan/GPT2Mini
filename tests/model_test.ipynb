{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11907312,"sourceType":"datasetVersion","datasetId":7485455},{"sourceId":407160,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":332687,"modelId":353615}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"440308ee","cell_type":"markdown","source":"## Load vocab","metadata":{}},{"id":"e5fa5059-9b52-48d1-a644-6b961b940f8c","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:38.572984Z","iopub.execute_input":"2025-05-22T11:10:38.573313Z","iopub.status.idle":"2025-05-22T11:10:38.584290Z","shell.execute_reply.started":"2025-05-22T11:10:38.573290Z","shell.execute_reply":"2025-05-22T11:10:38.583647Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gpt2mini/pytorch/default/1/decoder.py\n/kaggle/input/gpt2mini/pytorch/default/1/gpt2.py\n/kaggle/input/model-components/merges.txt\n/kaggle/input/model-components/vocab.json\n/kaggle/input/model-components/utils.py\n","output_type":"stream"}],"execution_count":18},{"id":"4848a39b-6b7d-42f4-9f5a-52d4dc5a64c8","cell_type":"code","source":"from datasets import load_dataset\n\n# Load and tokenize\ndataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\ntexts = [sample[\"text\"] for sample in dataset.select(range(50000))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:38.585240Z","iopub.execute_input":"2025-05-22T11:10:38.585511Z","iopub.status.idle":"2025-05-22T11:10:40.158679Z","shell.execute_reply.started":"2025-05-22T11:10:38.585483Z","shell.execute_reply":"2025-05-22T11:10:40.158094Z"}},"outputs":[],"execution_count":19},{"id":"0c00b1e3-62d1-4f06-8940-b872eb2a5d2c","cell_type":"code","source":"print(os.listdir(\"/kaggle/input\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.160747Z","iopub.execute_input":"2025-05-22T11:10:40.160955Z","iopub.status.idle":"2025-05-22T11:10:40.165371Z","shell.execute_reply.started":"2025-05-22T11:10:40.160940Z","shell.execute_reply":"2025-05-22T11:10:40.164687Z"}},"outputs":[{"name":"stdout","text":"['gpt2mini', 'model-components']\n","output_type":"stream"}],"execution_count":20},{"id":"ad570624","cell_type":"code","source":"import json\nwith open(\"/kaggle/input/model-components/vocab.json\") as f:\n    word2idx = json.load(f)\n    print(type(word2idx))\n    print(list(word2idx.items())[:10])\n\nidx2word = {int(v): k for k, v in word2idx.items()}\n# pad_id = word2idx[\"<pad>\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.166740Z","iopub.execute_input":"2025-05-22T11:10:40.167043Z","iopub.status.idle":"2025-05-22T11:10:40.205273Z","shell.execute_reply.started":"2025-05-22T11:10:40.167021Z","shell.execute_reply":"2025-05-22T11:10:40.204655Z"}},"outputs":[{"name":"stdout","text":"<class 'dict'>\n[('<s>', 0), ('<pad>', 1), ('</s>', 2), ('<unk>', 3), ('<mask>', 4), ('!', 5), ('\"', 6), ('#', 7), ('$', 8), ('%', 9)]\n","output_type":"stream"}],"execution_count":21},{"id":"27d3a734","cell_type":"code","source":"print(word2idx[\"cat\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.205902Z","iopub.execute_input":"2025-05-22T11:10:40.206148Z","iopub.status.idle":"2025-05-22T11:10:40.210000Z","shell.execute_reply.started":"2025-05-22T11:10:40.206123Z","shell.execute_reply":"2025-05-22T11:10:40.209283Z"}},"outputs":[{"name":"stdout","text":"9661\n","output_type":"stream"}],"execution_count":22},{"id":"b1f33bdd","cell_type":"markdown","source":"## Testing Tokenization","metadata":{}},{"id":"f48819df","cell_type":"code","source":"import sys\nsys.dont_write_bytecode = True # disabling __pycache__\nsys.path.insert(0, '/kaggle/input/model-components')\nfrom utils import Tokenizer\n# from utils import clean_text\n\ntokenizer = Tokenizer()\ntokenizer.upload_vocab(word2idx)\ntokenizer.encode(\"dog\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.210636Z","iopub.execute_input":"2025-05-22T11:10:40.210821Z","iopub.status.idle":"2025-05-22T11:10:40.227340Z","shell.execute_reply.started":"2025-05-22T11:10:40.210807Z","shell.execute_reply":"2025-05-22T11:10:40.226758Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[11902]"},"metadata":{}}],"execution_count":23},{"id":"c92d473d-91f4-4a3d-b4f1-c28ecd7b7c2e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"81dc9a4b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"43a74557","cell_type":"markdown","source":"## Testing the Model","metadata":{}},{"id":"a2c22ed9","cell_type":"code","source":"# embedding_dim == hidden_size == (D)\n# embedding_dim % num_heads == 0\nembedding_dim = 128\n\nff_embedding_dim = 512 # ff_embedding_dim = 4 Ã— embedding_dim\nmax_seq_len = 200\ndropout = 0.1\nnum_heads = 4\nvocab_size = tokenizer.get_vocab_size()\nnum_layers = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.228016Z","iopub.execute_input":"2025-05-22T11:10:40.228333Z","iopub.status.idle":"2025-05-22T11:10:40.243885Z","shell.execute_reply.started":"2025-05-22T11:10:40.228312Z","shell.execute_reply":"2025-05-22T11:10:40.243408Z"}},"outputs":[],"execution_count":24},{"id":"6e73f80c","cell_type":"code","source":"print(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.244573Z","iopub.execute_input":"2025-05-22T11:10:40.244807Z","iopub.status.idle":"2025-05-22T11:10:40.265558Z","shell.execute_reply.started":"2025-05-22T11:10:40.244791Z","shell.execute_reply":"2025-05-22T11:10:40.264975Z"}},"outputs":[{"name":"stdout","text":"19716\n","output_type":"stream"}],"execution_count":25},{"id":"1c03d0b1","cell_type":"code","source":"sys.path.insert(0, '/kaggle/input/gpt2mini/pytorch/default/1')\nfrom gpt2 import GPT2Model\nimport torch\nimport torch.nn as nn\n\nencoded = tokenizer.encode(\"dog\")\ninput_tensor = torch.tensor(encoded).unsqueeze(0)\n\nmodel = GPT2Model(vocab_size,embedding_dim,ff_embedding_dim,max_seq_len,num_heads,num_layers,dropout = 0.1)\n\n# for each position in the sequence, you get a distribution over all vocab tokens.\nlogits = model(input_tensor)  # (B, T, V)\n\n# Shift targets for next-token prediction\n# shift_logits = logits[:, :-1, :].contiguous()\n# shift_labels = input_tensor[:, 1:].contiguous()\n\n# Flatten for CrossEntropyLoss\n# loss_fn = nn.CrossEntropyLoss()\n# loss = loss_fn(\n#     shift_logits.view(-1, vocab_size),\n#     shift_labels.view(-1)\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.267179Z","iopub.execute_input":"2025-05-22T11:10:40.267398Z","iopub.status.idle":"2025-05-22T11:10:40.340993Z","shell.execute_reply.started":"2025-05-22T11:10:40.267382Z","shell.execute_reply":"2025-05-22T11:10:40.340419Z"}},"outputs":[],"execution_count":26},{"id":"5fb1ab18","cell_type":"code","source":"tokenized_text = tokenizer.encode(\"once day a time a cat\")\nif hasattr(tokenized_text, \"ids\"):\n    tokenized_text = tokenized_text.ids\nprint(tokenized_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.341818Z","iopub.execute_input":"2025-05-22T11:10:40.342398Z","iopub.status.idle":"2025-05-22T11:10:40.346281Z","shell.execute_reply.started":"2025-05-22T11:10:40.342373Z","shell.execute_reply":"2025-05-22T11:10:40.345721Z"}},"outputs":[{"name":"stdout","text":"[15976, 1131, 69, 3325, 69, 9661]\n","output_type":"stream"}],"execution_count":27},{"id":"14273844","cell_type":"code","source":"class CustomTextDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, tokenizer, seq_len):\n        self.data = []\n        self.seq_len = seq_len\n        self.tokenizer = tokenizer\n\n        for text in texts:\n            token_ids = self.tokenizer.encode(text)\n            if hasattr(token_ids, \"ids\"):  # in case tokenizer.encode returns an object\n                token_ids = token_ids.ids\n\n            for i in range(len(token_ids) - seq_len):\n                x = token_ids[i:i + seq_len]\n                y = token_ids[i + 1:i + 1 + seq_len]\n                self.data.append((x, y))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        x, y = self.data[idx]\n        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.346848Z","iopub.execute_input":"2025-05-22T11:10:40.347014Z","iopub.status.idle":"2025-05-22T11:10:40.366700Z","shell.execute_reply.started":"2025-05-22T11:10:40.347001Z","shell.execute_reply":"2025-05-22T11:10:40.366204Z"}},"outputs":[],"execution_count":28},{"id":"a8b329a5","cell_type":"markdown","source":"## Loading Data","metadata":{}},{"id":"8306b2f5","cell_type":"code","source":"from torch.utils.data import DataLoader\n# from data.dataset import TextDataset\n\n# Use longer sequences for testing\n\ndataset = CustomTextDataset(texts,tokenizer, seq_len=10)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:10:40.367519Z","iopub.execute_input":"2025-05-22T11:10:40.368102Z","iopub.status.idle":"2025-05-22T11:11:05.657138Z","shell.execute_reply.started":"2025-05-22T11:10:40.368058Z","shell.execute_reply":"2025-05-22T11:11:05.656275Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/3:   0%|          | 0/256946 [11:32<?, ?it/s, loss=3.36]\n","output_type":"stream"}],"execution_count":29},{"id":"5878e9d5","cell_type":"markdown","source":"## Training the Model","metadata":{}},{"id":"ae2a2761","cell_type":"code","source":"# embedding_dim == hidden_size == (D)\n# embedding_dim % num_heads == 0\nembedding_dim = 128\n\nff_embedding_dim = 512 # ff_embedding_dim = 4 Ã— embedding_dim\nmax_seq_len = 200\ndropout = 0.1\nnum_heads = 4\nvocab_size = tokenizer.get_vocab_size()\nnum_layers = 4\n\nepochs = 3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbatch_size = 16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:11:05.657987Z","iopub.execute_input":"2025-05-22T11:11:05.658780Z","iopub.status.idle":"2025-05-22T11:11:05.662973Z","shell.execute_reply.started":"2025-05-22T11:11:05.658756Z","shell.execute_reply":"2025-05-22T11:11:05.662263Z"}},"outputs":[],"execution_count":30},{"id":"227f32e1","cell_type":"code","source":"from tqdm import tqdm\nmodel = GPT2Model(\n        vocab_size,\n        embedding_dim,\n        ff_embedding_dim,max_seq_len\n        ,num_heads,\n        num_layers,\n        dropout = 0.1\n    ).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n\nloss_history = []\n\ndef train_loop(model, dataloader, criterion, optimizer, epochs, device, vocab_size):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for batch_idx, (x, y) in enumerate(dataloader):\n            x, y = x.to(device), y.to(device)  # x: [B, T], y: [B, T]\n\n            logits = model(x)  # [B, T, vocab_size]\n            logits = logits.view(-1, vocab_size)   # [B*T, V]\n            y = y.view(-1)                                # [B*T]\n\n            loss = criterion(logits, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()  # if used\n\n            total_loss += loss.item()\n            loss_history.append(loss.item())\n\n            # if batch_idx % 100 == 0:\n            #     print(f\"Epoch {epoch+1}, Step {batch_idx}, Loss: {loss.item():.4f}\")\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_loss = total_loss / len(dataloader)\n        print(f\"[{epoch+1:>3d}/{epochs:>3d}], Average loss:{avg_loss:>5f}\")\n\n    # Save model\n    # torch.save(model.state_dict(), \"gpt2_tiny.pth\")\n\ntrain_loop(model, dataloader, criterion, optimizer, epochs, device, vocab_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:11:05.663753Z","iopub.execute_input":"2025-05-22T11:11:05.663986Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/3:   0%|          | 0/256946 [03:37<?, ?it/s, loss=3.38]","output_type":"stream"}],"execution_count":null},{"id":"72624f35","cell_type":"markdown","source":"## Generate Text","metadata":{}},{"id":"edeb67ce","cell_type":"code","source":"# Load model\n# GPT2Config.vocab_size = len(word2idx)\n# model = GPT2Model(GPT2Config())\n# model.load_state_dict(torch.load(\"gpt2_tiny.pth\", map_location=\"cpu\"))\n# model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"945b2634","cell_type":"code","source":"# initialize tokenizer with texts\n\ndef generate_text(prompt, max_new_tokens=50):\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)  # [1, T]\n    \n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            logits = model(input_tensor)  # [1, T, vocab]\n            next_token_logits = logits[:, -1, :]  # last position\n            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # [1, 1]\n            input_tensor = torch.cat([input_tensor, next_token], dim=1)  # grow the sequence\n\n    return tokenizer.decode(input_tensor[0].tolist())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"60b63b0e","cell_type":"code","source":"prompt = \"Once upon a time\"\nprint(generate_text(prompt, max_new_tokens=16))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}