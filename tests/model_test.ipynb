{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11907312,"sourceType":"datasetVersion","datasetId":7485455},{"sourceId":407160,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":332687,"modelId":353615}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"440308ee","cell_type":"markdown","source":"## Load vocab","metadata":{}},{"id":"e5fa5059-9b52-48d1-a644-6b961b940f8c","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:21:14.596184Z","iopub.execute_input":"2025-05-22T15:21:14.596848Z","iopub.status.idle":"2025-05-22T15:21:14.603159Z","shell.execute_reply.started":"2025-05-22T15:21:14.596817Z","shell.execute_reply":"2025-05-22T15:21:14.602433Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gpt2mini/pytorch/default/1/decoder.py\n/kaggle/input/gpt2mini/pytorch/default/1/gpt2.py\n/kaggle/input/model-components/merges.txt\n/kaggle/input/model-components/vocab.json\n/kaggle/input/model-components/utils.py\n","output_type":"stream"}],"execution_count":85},{"id":"4848a39b-6b7d-42f4-9f5a-52d4dc5a64c8","cell_type":"code","source":"from datasets import load_dataset\n\n# Load and tokenize\ndataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\ntexts = [sample[\"text\"] for sample in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:38:32.787691Z","iopub.execute_input":"2025-05-22T16:38:32.787999Z","iopub.status.idle":"2025-05-22T16:39:05.420048Z","shell.execute_reply.started":"2025-05-22T16:38:32.787977Z","shell.execute_reply":"2025-05-22T16:39:05.419449Z"}},"outputs":[],"execution_count":242},{"id":"0c00b1e3-62d1-4f06-8940-b872eb2a5d2c","cell_type":"code","source":"print(os.listdir(\"/kaggle/input\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:06.289771Z","iopub.execute_input":"2025-05-22T16:28:06.290016Z","iopub.status.idle":"2025-05-22T16:28:06.294627Z","shell.execute_reply.started":"2025-05-22T16:28:06.290000Z","shell.execute_reply":"2025-05-22T16:28:06.293867Z"}},"outputs":[{"name":"stdout","text":"['gpt2mini', 'model-components']\n","output_type":"stream"}],"execution_count":217},{"id":"ad570624","cell_type":"code","source":"import json\nwith open(\"/kaggle/input/model-components/vocab.json\") as f:\n    word2idx = json.load(f)\n    print(type(word2idx))\n    print(list(word2idx.items())[:10])\n\nidx2word = {int(v): k for k, v in word2idx.items()}\n# pad_id = word2idx[\"<pad>\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:08.385875Z","iopub.execute_input":"2025-05-22T16:28:08.386146Z","iopub.status.idle":"2025-05-22T16:28:08.415783Z","shell.execute_reply.started":"2025-05-22T16:28:08.386125Z","shell.execute_reply":"2025-05-22T16:28:08.415191Z"}},"outputs":[{"name":"stdout","text":"<class 'dict'>\n[('<s>', 0), ('<pad>', 1), ('</s>', 2), ('<unk>', 3), ('<mask>', 4), ('!', 5), ('\"', 6), ('#', 7), ('$', 8), ('%', 9)]\n","output_type":"stream"}],"execution_count":218},{"id":"27d3a734","cell_type":"code","source":"print(word2idx[\"cat\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:10.556650Z","iopub.execute_input":"2025-05-22T16:28:10.556910Z","iopub.status.idle":"2025-05-22T16:28:10.561005Z","shell.execute_reply.started":"2025-05-22T16:28:10.556891Z","shell.execute_reply":"2025-05-22T16:28:10.560300Z"}},"outputs":[{"name":"stdout","text":"9661\n","output_type":"stream"}],"execution_count":219},{"id":"b1f33bdd","cell_type":"markdown","source":"## Testing Tokenization","metadata":{}},{"id":"f48819df","cell_type":"code","source":"import sys\nsys.dont_write_bytecode = True # disabling __pycache__\nsys.path.insert(0, '/kaggle/input/model-components')\nfrom utils import Tokenizer\n# from utils import clean_text\n\ntokenizer = Tokenizer()\ntokenizer.upload_vocab(word2idx)\ntokenizer.encode(\"dog\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:12.556934Z","iopub.execute_input":"2025-05-22T16:28:12.557179Z","iopub.status.idle":"2025-05-22T16:28:12.563763Z","shell.execute_reply.started":"2025-05-22T16:28:12.557164Z","shell.execute_reply":"2025-05-22T16:28:12.563049Z"}},"outputs":[{"execution_count":220,"output_type":"execute_result","data":{"text/plain":"[11902]"},"metadata":{}}],"execution_count":220},{"id":"43a74557","cell_type":"markdown","source":"## Testing the Model","metadata":{}},{"id":"a2c22ed9","cell_type":"code","source":"# embedding_dim == hidden_size == (D)\n# embedding_dim % num_heads == 0\nembedding_dim = 64\n\nff_embedding_dim = 128 # ff_embedding_dim = 4 × embedding_dim\nmax_seq_len = 10\ndropout = 0.1\nnum_heads = 2\nvocab_size = tokenizer.get_vocab_size()\nnum_layers = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:14.676088Z","iopub.execute_input":"2025-05-22T16:28:14.676321Z","iopub.status.idle":"2025-05-22T16:28:14.680069Z","shell.execute_reply.started":"2025-05-22T16:28:14.676306Z","shell.execute_reply":"2025-05-22T16:28:14.679509Z"}},"outputs":[],"execution_count":221},{"id":"6e73f80c","cell_type":"code","source":"print(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:16.721982Z","iopub.execute_input":"2025-05-22T16:28:16.722522Z","iopub.status.idle":"2025-05-22T16:28:16.726089Z","shell.execute_reply.started":"2025-05-22T16:28:16.722502Z","shell.execute_reply":"2025-05-22T16:28:16.725482Z"}},"outputs":[{"name":"stdout","text":"19716\n","output_type":"stream"}],"execution_count":222},{"id":"1c03d0b1","cell_type":"code","source":"sys.path.insert(0, '/kaggle/input/gpt2mini/pytorch/default/1')\nfrom gpt2 import GPT2Model\nimport torch\nimport torch.nn as nn\n\nencoded = tokenizer.encode(\"dog\")\ninput_tensor = torch.tensor(encoded).unsqueeze(0)\n\nmodel = GPT2Model(vocab_size,embedding_dim,ff_embedding_dim,max_seq_len,num_heads,num_layers,dropout = 0.1)\n\n# for each position in the sequence, you get a distribution over all vocab tokens.\nlogits = model(input_tensor)  # (B, T, V)\n\n# Shift targets for next-token prediction\n# shift_logits = logits[:, :-1, :].contiguous()\n# shift_labels = input_tensor[:, 1:].contiguous()\n\n# Flatten for CrossEntropyLoss\n# loss_fn = nn.CrossEntropyLoss()\n# loss = loss_fn(\n#     shift_logits.view(-1, vocab_size),\n#     shift_labels.view(-1)\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:18.734278Z","iopub.execute_input":"2025-05-22T16:28:18.734509Z","iopub.status.idle":"2025-05-22T16:28:18.768881Z","shell.execute_reply.started":"2025-05-22T16:28:18.734493Z","shell.execute_reply":"2025-05-22T16:28:18.768048Z"}},"outputs":[],"execution_count":223},{"id":"5fb1ab18","cell_type":"code","source":"tokenized_text = tokenizer.encode(\"once day a time a cat\")\nif hasattr(tokenized_text, \"ids\"):\n    tokenized_text = tokenized_text.ids\nprint(tokenized_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:21.068077Z","iopub.execute_input":"2025-05-22T16:28:21.068338Z","iopub.status.idle":"2025-05-22T16:28:21.072771Z","shell.execute_reply.started":"2025-05-22T16:28:21.068303Z","shell.execute_reply":"2025-05-22T16:28:21.072003Z"}},"outputs":[{"name":"stdout","text":"[15976, 1131, 69, 3325, 69, 9661]\n","output_type":"stream"}],"execution_count":224},{"id":"7f9aa510-45a4-4d0d-a422-f2496ec965fd","cell_type":"code","source":"tokenizer.encode(\"<pad>\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:23.146031Z","iopub.execute_input":"2025-05-22T16:28:23.146284Z","iopub.status.idle":"2025-05-22T16:28:23.151276Z","shell.execute_reply.started":"2025-05-22T16:28:23.146266Z","shell.execute_reply":"2025-05-22T16:28:23.150523Z"}},"outputs":[{"execution_count":225,"output_type":"execute_result","data":{"text/plain":"[1]"},"metadata":{}}],"execution_count":225},{"id":"14273844","cell_type":"code","source":"class CustomTextDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, tokenizer, seq_len):\n        self.texts = texts\n        self.seq_len = seq_len\n        self.tokenizer = tokenizer\n        self.pad_id = word2idx[\"<pad>\"]  # Make sure you have a padding token\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # Tokenize and convert to IDs\n        token_ids = self.tokenizer.encode(text)\n        if hasattr(token_ids, \"ids\"):\n            token_ids = token_ids.ids\n        \n        # Handle sequences that are too short or too long\n        if len(token_ids) >= self.seq_len:\n            # Take the first seq_len tokens if too long\n            token_ids = token_ids[:self.seq_len]\n        else:\n            # Pad with <pad> tokens if too short\n            padding = [self.pad_id] * (self.seq_len - len(token_ids))\n            token_ids = token_ids + padding\n        \n        # Create input and target sequences\n        x = torch.tensor(token_ids[:-1], dtype=torch.long)  # Input sequence\n        y = torch.tensor(token_ids[1:], dtype=torch.long)   # Target sequence\n        \n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:24.833013Z","iopub.execute_input":"2025-05-22T16:28:24.833281Z","iopub.status.idle":"2025-05-22T16:28:24.839838Z","shell.execute_reply.started":"2025-05-22T16:28:24.833262Z","shell.execute_reply":"2025-05-22T16:28:24.838901Z"}},"outputs":[],"execution_count":226},{"id":"a8b329a5","cell_type":"markdown","source":"## Loading Data","metadata":{}},{"id":"8306b2f5","cell_type":"code","source":"from torch.utils.data import DataLoader\n# from data.dataset import TextDataset\n\n# Use longer sequences for testing\n\ndataset = CustomTextDataset(texts,tokenizer, seq_len=10)\ndataloader = DataLoader(\n    dataset, \n    batch_size=16,  # Reduced from 32\n    shuffle=True, \n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:28.271077Z","iopub.execute_input":"2025-05-22T16:28:28.271353Z","iopub.status.idle":"2025-05-22T16:28:28.288933Z","shell.execute_reply.started":"2025-05-22T16:28:28.271335Z","shell.execute_reply":"2025-05-22T16:28:28.288310Z"}},"outputs":[],"execution_count":227},{"id":"5878e9d5","cell_type":"markdown","source":"## Training the Model","metadata":{}},{"id":"ae2a2761","cell_type":"code","source":"# embedding_dim == hidden_size == (D)\n# embedding_dim % num_heads == 0\nembedding_dim = 64\n\nff_embedding_dim = 128 # ff_embedding_dim = 4 × embedding_dim\nmax_seq_len = 10\ndropout = 0.1\nnum_heads = 2\nvocab_size = tokenizer.get_vocab_size()\nnum_layers = 2\n\nepochs = 50\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbatch_size = 16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:28:30.799917Z","iopub.execute_input":"2025-05-22T16:28:30.800602Z","iopub.status.idle":"2025-05-22T16:28:30.804516Z","shell.execute_reply.started":"2025-05-22T16:28:30.800571Z","shell.execute_reply":"2025-05-22T16:28:30.803882Z"}},"outputs":[],"execution_count":228},{"id":"005c656a-8ee0-4fc5-9d48-550408ffa76b","cell_type":"code","source":"x, y = next(iter(dataloader))\n#-----Important-------#\n# if this cell causes device error comment or remove the comment for the line bellow\nx, y = x.to(device), y.to(device)\nlogits = model(x)\nprint(\"Logits shape:\", logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:40:12.475782Z","iopub.execute_input":"2025-05-22T16:40:12.476058Z","iopub.status.idle":"2025-05-22T16:40:12.489204Z","shell.execute_reply.started":"2025-05-22T16:40:12.476039Z","shell.execute_reply":"2025-05-22T16:40:12.488475Z"}},"outputs":[{"name":"stdout","text":"Logits shape: torch.Size([16, 9, 19716])\n","output_type":"stream"}],"execution_count":244},{"id":"227f32e1","cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\ndef train_model_with_checkpoints(\n    model, \n    dataloader, \n    criterion, \n    optimizer, \n    epochs, \n    device, \n    vocab_size, \n    pad_id, \n    checkpoint_dir=\"model-components/checkpoints\",\n    checkpoint_freq=1  # Save every N epochs\n):\n    # Create checkpoint directory\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    model = model.to(device)\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'lr_history': []\n    }\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        epoch_correct = 0\n        epoch_total = 0\n        \n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for batch_idx, (x, y) in enumerate(progress_bar):\n            x, y = x.to(device), y.to(device)\n            \n            # Forward pass\n            logits = model(x)\n            \n            # Calculate metrics\n            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n            preds = logits.argmax(dim=-1)\n            mask = y != pad_id\n            correct = (preds[mask] == y[mask]).float().sum()\n            total = mask.float().sum()\n            acc = correct / total if total > 0 else 0\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # Update metrics\n            epoch_loss += loss.item()\n            epoch_correct += correct.item()\n            epoch_total += total.item()\n            \n            progress_bar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'acc': f\"{acc.item():.2%}\"\n            })\n        \n        # Save epoch metrics\n        avg_loss = epoch_loss / len(dataloader)\n        avg_acc = epoch_correct / epoch_total if epoch_total > 0 else 0\n        history['train_loss'].append(avg_loss)\n        history['train_acc'].append(avg_acc)\n        history['lr_history'].append(optimizer.param_groups[0]['lr'])\n        \n        # Save checkpoint\n        if (epoch + 1) % checkpoint_freq == 0:\n            checkpoint_path = os.path.join(\n                checkpoint_dir, \n                f\"checkpoint_epoch_{epoch+1}.pt\"\n            )\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_loss,\n                'accuracy': avg_acc,\n                'history': history\n            }, checkpoint_path)\n            print(f\"Saved checkpoint to {checkpoint_path}\")\n        \n        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Acc: {avg_acc:.2%}\")\n    \n    return model, history\n\n\n# Example usage:\ntrained_model, training_history = train_model(model, dataloader, criterion, \n                                           optimizer, epochs, device, \n                                           vocab_size,1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:48:52.520513Z","iopub.execute_input":"2025-05-22T16:48:52.520832Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/50:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0bc03fda0314354ad7dd78afe46f075"}},"metadata":{}}],"execution_count":null},{"id":"72624f35","cell_type":"markdown","source":"## Generate Text","metadata":{}},{"id":"edeb67ce","cell_type":"code","source":"# Load model\n# GPT2Config.vocab_size = len(word2idx)\n# model = GPT2Model(GPT2Config())\n# model.load_state_dict(torch.load(\"gpt2_tiny.pth\", map_location=\"cpu\"))\n# model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:18:31.986981Z","iopub.status.idle":"2025-05-22T15:18:31.987290Z","shell.execute_reply.started":"2025-05-22T15:18:31.987134Z","shell.execute_reply":"2025-05-22T15:18:31.987149Z"}},"outputs":[],"execution_count":null},{"id":"945b2634","cell_type":"code","source":"# initialize tokenizer with texts\n\ndef generate_text(prompt, max_new_tokens=50):\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)  # [1, T]\n    \n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            logits = model(input_tensor)  # [1, T, vocab]\n            next_token_logits = logits[:, -1, :]  # last position\n            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # [1, 1]\n            input_tensor = torch.cat([input_tensor, next_token], dim=1)  # grow the sequence\n\n    return tokenizer.decode(input_tensor[0].tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:18:31.988362Z","iopub.status.idle":"2025-05-22T15:18:31.988652Z","shell.execute_reply.started":"2025-05-22T15:18:31.988478Z","shell.execute_reply":"2025-05-22T15:18:31.988487Z"}},"outputs":[],"execution_count":null},{"id":"60b63b0e","cell_type":"code","source":"prompt = \"Once upon a time\"\nprint(generate_text(prompt, max_new_tokens=16))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:18:31.989861Z","iopub.status.idle":"2025-05-22T15:18:31.990101Z","shell.execute_reply.started":"2025-05-22T15:18:31.989984Z","shell.execute_reply":"2025-05-22T15:18:31.989997Z"}},"outputs":[],"execution_count":null}]}